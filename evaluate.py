#!/usr/bin/env python
"""evaluate.py
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π (DeepFace vs OpenAI GPT-4 Vision)

–ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤:
  python evaluate.py --mode local  --csv ck_paths.csv --root ck_videos
  python evaluate.py --mode cloud  --csv ck_paths.csv --root ck_videos --api_key $OPENAI_API_KEY
"""
from __future__ import annotations

import argparse, pathlib, time, base64, sys, json
from typing import List

import pandas as pd
import numpy as np
import cv2
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from tqdm import tqdm

# ---------------------------- Predictors --------------------------------- #
class Predictor:
    """–ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å."""
    name: str = "base"

    def predict(self, frame) -> str:
        raise NotImplementedError

class LocalPredictor(Predictor):
    """DeepFace predictor"""
    name = "local"

    def __init__(self, detector_backend: str = "retinaface", enforce: bool = True):
        """detector_backend: retinaface, opencv, mtcnn, skip –∏ –¥—Ä.
        enforce: –µ—Å–ª–∏ False, DeepFace –Ω–µ –±—É–¥–µ—Ç —Ä—É–≥–∞—Ç—å—Å—è –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ª–∏—Ü.
        """
        from deepface import DeepFace  # –∏–º–ø–æ—Ä—Ç –≤–Ω—É—Ç—Ä–∏, —á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–∏ cloud
        self._backend = DeepFace
        self._actions = ["emotion"]
        self._detector_backend = detector_backend
        self._enforce = enforce

    def predict(self, frame):
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è —Å –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–º –ª–∏—Ü (—Ç–æ—á–Ω–µ–µ)
        try:
            res = self._backend.analyze(
                frame,
                actions=["emotion"],
                detector_backend=self._detector_backend,
                enforce_detection=self._enforce,
            )
        except Exception:
            # fallback: –±–µ–∑ –¥–µ—Ç–µ–∫—Ü–∏–∏ (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç neutral, –Ω–æ –ª—É—á—à–µ, —á–µ–º –æ—à–∏–±–∫–∞)
            res = self._backend.analyze(
                frame,
                actions=["emotion"],
                detector_backend="skip",
                enforce_detection=False,
            )
        emotions = res[0]["emotion"]
        top_label = max(emotions, key=emotions.get).lower()
        confidence = emotions[top_label]
        return top_label, confidence

class CloudPredictor(Predictor):
    """OpenAI Vision predictor"""
    name = "cloud"

    def __init__(self, api_key: str):
        from openai import OpenAI
        self._client = OpenAI(api_key=api_key)

    _PROMPT = (
        "–û–ø—Ä–µ–¥–µ–ª–∏ —ç–º–æ—Ü–∏—é —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –û—Ç–≤–µ—Ç—å –û–î–ù–ò–ú –°–õ–û–í–û–ú "
        "–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏–∑ —Å–ø–∏—Å–∫–∞: angry, disgust, fear, happy, neutral, sad, surprise. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π."
    )

    def predict(self, frame):
        _, buf = cv2.imencode(".jpg", frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
        img64 = base64.b64encode(buf).decode()
        resp = self._client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self._PROMPT},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img64}"}},
                    ],
                }
            ],
            max_tokens=5,
            temperature=0.1,
        )
        raw = resp.choices[0].message.content.strip().lower()
        label = raw.split()[0]          # –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
        allowed = {"angry","disgust","fear","happy","neutral","sad","surprise"}
        if label not in allowed:
            label = "neutral"           # –∏–ª–∏ label = "unknown" –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å
        return label, 1.0

# ------------------------------------------------------------------------- #

# ----------- Pre-processing (upsample & grayscale‚ÜíRGB) -------------------- #

def prepare_frame(frame, target: int = 224):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫ 3-–∫–∞–Ω–∞–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –¥–æ target px."""
    global _SKIP_PREPROCESS
    if _SKIP_PREPROCESS:
        return frame
    if frame is None:
        return frame
    # grayscale ‚Üí BGR
    if len(frame.shape) == 2 or frame.shape[2] == 1:
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
    h, w = frame.shape[:2]
    if min(h, w) < target:
        frame = cv2.resize(frame, (target, target), interpolation=cv2.INTER_CUBIC)
    return frame

# ------------- helpers ---------------------------------------------------- #

def get_first_frame(cap: cv2.VideoCapture):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä —Ä–æ–ª–∏–∫–∞ (pos=0)."""
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    _, frame = cap.read()
    return frame


def get_frames(cap: cv2.VideoCapture, max_frames:int=3):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ max_frames –∫–∞–¥—Ä–æ–≤, —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø–æ —Ä–æ–ª–∏–∫—É."""
    frames_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 1
    step = max(frames_total // max_frames, 1)
    frames = []
    for i in range(0, frames_total, step):
        if len(frames) >= max_frames:
            break
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frames.append(frame)
    return frames or [cap.read()[1]]  # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –≤–µ—Ä–Ω—ë–º –ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä

def get_last_frame(cap):
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.set(cv2.CAP_PROP_POS_FRAMES, total-1)
    _, f = cap.read();  return f


def evaluate(
    csv_path: pathlib.Path,
    root: pathlib.Path,
    predictor: Predictor,
    show_details: bool = False,
    max_per_class: int = 0,
    conf_thres: float = 0.5,
    exclude_neutral: bool = False,
):
    df = pd.read_csv(csv_path)
    if not {"path", "label"}.issubset(df.columns):
        print("CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã path,label", file=sys.stderr)
        sys.exit(1)

    # --- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä–∫—É: –Ω–µ –±–æ–ª–µ–µ max_per_class –≤–∏–¥–µ–æ –Ω–∞ —ç–º–æ—Ü–∏—é ---
    if max_per_class > 0:
        df = df.groupby('label', as_index=False).head(max_per_class).reset_index(drop=True)

    y_true: List[str] = []
    y_pred: List[str] = []
    latencies: List[float] = []

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="–í–∏–¥–µ–æ—Ä–æ–ª–∏–∫–∏", unit="vid"):
        vid_path = root / row["path"]
        label = str(row["label"]).lower()
        if not vid_path.exists():
            print(f"‚ö†Ô∏è  —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {vid_path}")
            continue
        cap = cv2.VideoCapture(str(vid_path))
        if not cap.isOpened():
            print(f"‚ö†Ô∏è  –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å: {vid_path}")
            continue

        frame = get_first_frame(cap)
        cap.release()

        if (idx % 50) == 0 and idx:
            print(f"üîÑ  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx} –≤–∏–¥–µ–æ")

        fr_prep = prepare_frame(frame)
        t0 = time.perf_counter()
        pred, conf = predictor.predict(fr_prep)
        lat = (time.perf_counter() - t0) * 1000

        # –º–∞–ø–ø–∏–Ω–≥ –∏—Å—Ç–∏–Ω–Ω–æ–π –º–µ—Ç–∫–∏ –∫ —Ñ–æ—Ä–º–∞—Ç—É deepface
        map_ck_to_df = {
            "anger": "angry",
            "sadness": "sad",  # CK+ sadness -> DeepFace sad
        }
        label_mapped = map_ck_to_df.get(label, label)

        # –¥–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –ø–µ—Ä–µ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        if show_details:
            tqdm.write(
                f"{vid_path.name}: true={label.lower()} mapped={label_mapped} | pred={pred} conf={conf:.3f}"
            )

        # —Ñ–∏–ª—å—Ç—Ä—ã
        if conf < conf_thres:
            continue
        if exclude_neutral and (label_mapped == "neutral" or pred == "neutral"):
            continue

        y_true.append(label_mapped)
        y_pred.append(pred)
        latencies.append(lat)

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="macro")
    avg_lat = float(np.mean(latencies))
    p90 = float(np.percentile(latencies, 90))

    results = {
        "model": predictor.name,
        "samples": len(y_true),
        "accuracy": round(acc * 100, 2),
        "macro_f1": round(f1, 3),
        "latency_ms_avg": round(avg_lat, 1),
        "latency_ms_p90": round(p90, 1),
    }
    print(json.dumps(results, ensure_ascii=False, indent=2))

    # --- –ø–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ---
    try:
        print("\nClassification report:\n", classification_report(y_true, y_pred, digits=2))
        labels_sorted = sorted(set(y_true) | set(y_pred))
        cm = confusion_matrix(y_true, y_pred, labels=labels_sorted)
        print("Confusion matrix (rows=true, cols=pred):")
        # –ø–µ—á–∞—Ç–∞–µ–º –∫—Ä–∞—Å–∏–≤–µ–µ
        header = "        " + "  ".join(f"{l:>8}" for l in labels_sorted)
        print(header)
        for lbl, row_cm in zip(labels_sorted, cm):
            row_str = "  ".join(f"{v:8d}" for v in row_cm)
            print(f"{lbl:>8}  {row_str}")
    except Exception as e:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:", e)


def main():
    ap = argparse.ArgumentParser(description="Evaluate emotion recognition models")
    ap.add_argument("--csv", default="ckextended.csv", help="CSV file with path,label columns")
    ap.add_argument("--root", default="ck_videos", help="Root directory with videos")
    ap.add_argument("--mode", choices=["local", "cloud"], required=True)
    ap.add_argument("--api_key", help="OpenAI API key (for cloud mode)")
    ap.add_argument("--details", action="store_true", help="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–µ–æ")
    ap.add_argument("--backend", default="retinaface", help="DeepFace detector backend (retinaface|opencv|mtcnn|skip)")
    ap.add_argument("--no_enforce", action="store_true", help="–ù–µ —Ç—Ä–µ–±–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏—è –ª–∏—Ü–∞ (enforce_detection=False)")
    ap.add_argument("--list_emotions", action="store_true", help="–í—ã–≤–µ—Å—Ç–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ DeepFace —ç–º–æ—Ü–∏–∏ –∏ –≤—ã–π—Ç–∏")
    ap.add_argument("--no_preprocess", action="store_true", help="–ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–¥—Ä–æ–≤")
    ap.add_argument("--max_per_class", type=int, default=10, help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–µ–æ –Ω–∞ —ç–º–æ—Ü–∏—é (0 - –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å)")
    ap.add_argument("--conf_thres", type=float, default=0.5, help="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ DeepFace (prediction confidence)")
    ap.add_argument("--exclude_neutral", action="store_true", help="–ò—Å–∫–ª—é—á–∏—Ç—å neutral –∏–∑ —Ä–∞—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫")
    args = ap.parse_args()

    csv_path = pathlib.Path(args.csv)
    root = pathlib.Path(args.root)

    if args.list_emotions:
        # –≤—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ —ç–º–æ—Ü–∏–π DeepFace –∏ –≤—ã—Ö–æ–¥–∏–º
        from deepface import DeepFace
        import numpy as _np
        dummy = _np.zeros((10, 10, 3), dtype=_np.uint8)
        emotions = DeepFace.analyze(dummy, actions=["emotion"], enforce_detection=False)[0]["emotion"].keys()
        print("DeepFace emotions:", ", ".join(emotions))
        return

    if args.mode == "local":
        predictor = LocalPredictor(detector_backend=args.backend, enforce=not args.no_enforce)
    else:
        if not args.api_key:
            print("--api_key required for cloud mode", file=sys.stderr)
            sys.exit(1)
        predictor = CloudPredictor(args.api_key)

    global _SKIP_PREPROCESS
    _SKIP_PREPROCESS = args.no_preprocess

    evaluate(
        csv_path,
        root,
        predictor,
        show_details=args.details,
        max_per_class=args.max_per_class,
        conf_thres=args.conf_thres,
        exclude_neutral=args.exclude_neutral,
    )


if __name__ == "__main__":
    main() 